{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 先后阅读若干AlexNet的各种详解,在AlexNet模块上进行输入和末尾输出的更改.\n",
    "\n",
    "\n",
    "> [TensorFlow（1）-AlexNet实现(知乎)](https://zhuanlan.zhihu.com/p/27381582)\n",
    "\n",
    "> [TensorFlow实战之实现AlexNet经典卷积神经网络](http://www.cnblogs.com/georgeli/p/8476307.html)\n",
    "\n",
    "> [规范模型1](https://github.com/qiansi/tensorflow-AlexNet/blob/master/AlexNet.py)\n",
    "\n",
    "> [规范模型2](https://github.com/yqtaowhu/MachineLearning/blob/master/AlexNet/alexNet.py)\n",
    "\n",
    "> [【卷积神经网络-进化史】从LeNet到AlexNet](https://blog.csdn.net/cyh_24/article/details/51440344)\n",
    "\n",
    "> [干货|详解CNN五大经典模型:Lenet，Alexnet，Googlenet，VGG，DRL](http://www.sohu.com/a/134347664_642762)\n",
    "\n",
    "> [TF-api(2) tf.nn.lrn (以下1.2.2.3)](https://blog.csdn.net/GZHermit/article/details/75389130)\n",
    "\n",
    "> [AlexNet原理及Tensorflow实现](https://blog.csdn.net/taoyanqi8932/article/details/71081390)\n",
    "\n",
    "> [我看AlexNet](https://www.jianshu.com/p/58168fec534d)\n",
    "\n",
    "## AlexNet网络优点总结：\n",
    "    AlexNet之所以能够成功，深度学习之所以能够重回历史舞台，原因在于：\n",
    "\t1.非线性激活函数：ReLU\n",
    "\t2.防止过拟合的方法：Dropout，Data augmentation\n",
    "\t3.大数据训练：百万级ImageNet图像数据\n",
    "\t4.其他：GPU实现，LRN归一化层的使用\n",
    "    \n",
    "## 近期学习总结:（4.15-4.28）\n",
    "    1.Softmax函数 4.15\n",
    "    2.Logistics Regression推导 4.16\n",
    "        2.1损失函数:二次代价函数与交叉熵函数;\n",
    "        2.2梯度下降;\n",
    "        2.3损失函数的推导\n",
    "    3.激活函数的理解，Sigmoid、ReLu、Tanh、Leaky ReLu，以及正向传播和反向传播 4.17\n",
    "    4.非线性回归的代码实现，以及MNIST数据简单分类的代码练习 4.19\n",
    "    5.对数似然函数的softmax回归的代价函数，超参数的定义以及过拟合 4.21\n",
    "    6.Dropout以及Optimizer的代码实现 4.22\n",
    "    7.CNN的工作方式，（卷积核、Padding、Stride）简单的CNN模型复现 4.23\n",
    "        7.1个人理解，假设输入是一张图片，CNN就是提取图片的特征，输出为Feature Map，减少计算量同时特征保留。例如输入为39*39*3，输出为7*7*4，减少很多的计算量\n",
    "        7.2通常一个卷积层: \n",
    "   > input ---> conv ---> pool ---> FC ---> output\n",
    "   \n",
    "        7.3池化层;\n",
    "        7.4超参的选择，基本选取其他人设定好的参数;\n",
    "        7.5卷积层的优势;\n",
    "        7.6LeNet-5、AlexNet、VGG-16、ResNet的网络结构原理\n",
    "    8.Tensorboard代码实现，暂时只学习了:4.24\n",
    "   > with tensorflow.name_scope() as scope:\n",
    "    \n",
    "        的用法，将学习:\n",
    "   > tensorflow.summary.merge_all()\n",
    "    \n",
    "        8.1 ssh远程实现Jupyter Notebook:        \n",
    "```bash\n",
    "    ssh username@host address -N -L loacalhost:8889:loacalhost:8888\n",
    "```\n",
    "    9.CNN提高MNIST的识别准确率代码实现1，参考: 4.25\n",
    "   > [MNIST wiki](https://wiki.jikexueyuan.com/project/tensorflow-zh/tutorials/mnist/download/index.html)\n",
    "        \n",
    "        9.1 下载数据集；\n",
    "        9.2 tf.argmax();\n",
    "        9.3 tf.equal();\n",
    "        9.4 tf.cast();\n",
    "        9.5 tf.nn.conv2d();\n",
    "        9.6 一层卷积，两个卷积核的图片推导;\n",
    "        9.7 卷积各种参数的推导,比如tf.nn.conv2d()内的参数推导，tf.nn.max_pool()内的参数推导\n",
    "    10.CNN提高MNIST的识别准确率代码实现2 4.26\n",
    "        10.1 数据集\n",
    "        10.2 权值、偏置、卷积层、池化层函数\n",
    "        10.3 损失函数、优化器\n",
    "        10.4 结构:\n",
    "   > input ---> conv1 -> max_pool1---> conv2 -> max_pool2 ---> FC1 ---> FC2 --->output\n",
    "    \n",
    "    11.手动推算AlexNet网络结构的各种参数 4.28\n",
    "        11.1 AlexNet的优点，上述已总结\n",
    "        11.2 初步推导AlexNet网络结构，数据集用的是MNIST数据集\n",
    "    \n",
    "## 未能搞明白的地方:\n",
    "    1.各种优化器的具体参数推导，比如Adam、Moment等;\n",
    "    2.AlexNet网络中的LRN层，资料显示是归一化层。\n",
    "    3.作用:\n",
    "  \n",
    "        ①使响应比较大的值相对更大，比较小的值相对更小，提高模型的泛化能力。本质上，这个层也是为了防止激活函数的饱和的;\n",
    "        ②对channel做计算;\n",
    "        ③2015年 Very Deep Convolutional Networks for Large-Scale Image Recognition.这篇文章提到LRN基本没什么用，反而还增加了计算时间;\n",
    "    4.有可能batch normalization替代了lrn，故对此层没有深入研究，仅了解如下:\n",
    "        ①平滑输出，输出前后shape不变;\n",
    "        ②只是一个公式,了解各个参数代表的意义;\n",
    "![LRN](https://upload-images.jianshu.io/upload_images/1689929-a119df5b00736da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700 \"lrn\")\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    利用AlexNet网络实现MNIST准确识别。\n",
    "    没有GPU，调整了卷积核的参数，并在每一层卷积和全连接层之后增加了一层Dropout层。\n",
    "    \n",
    "    运行下面这段代码的时候，抛出错误：\n",
    "        The kernel appears to have died. It will restart automatically.\n",
    "        翻译：核心似乎已经死亡。 它会自动重启。\n",
    "        还没找到解决的有效方法，在命令行上执行的结果是：能执行代码，并没有推论中的输出的准确率的判断。\n",
    "'''\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/',one_hot=True)\n",
    "\n",
    "# 每个批次的大小\n",
    "batch_size = 100\n",
    "# 总共多少批次\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# 初始化权值，偏置，卷积层，池化层,lrn层\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,dtype=tf.float32,stddev=1e-1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0,shape=shape,dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 原strides=[1,4,4,1]\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "# 原ksize=[1,3,3,1],strides=[1,2,2,1]\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "\n",
    "# 这儿的参数是抄的，别问我，我也不知道怎么推算出来的，基本上的AlexNet都在用同一个数据\n",
    "def lrn(x):\n",
    "    return tf.nn.lrn(x,4,bias=0.001/9.0,beta=0.75)\n",
    "\n",
    "# 定义placeholder,占位符\n",
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,10])\n",
    "lr = tf.Variable(0.001,dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# x --> 4维向量\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "# 输入是28*28*1，conv1，图片比较小，正宗用的是224*224，我用的是28*28\n",
    "W_conv1 = weight_variable([5,5,1,32]) \n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_lrn1 = lrn(h_pool1)\n",
    "# 只有CPU，没有GPU加速，所以丢了个dropout在每一层\n",
    "h_drop1 = tf.nn.dropout(h_lrn1,keep_prob)\n",
    "\n",
    "\n",
    "# 上一层输出是14*14*32，conv2\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_drop1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "h_lrn2 = lrn(h_pool2)\n",
    "h_drop2 = tf.nn.dropout(h_lrn2,keep_prob)\n",
    "\n",
    "# 上一层输出是7*7*64,conv3\n",
    "W_conv3 = weight_variable([5,5,64,128])\n",
    "b_conv3 = bias_variable([128])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_drop2,W_conv3) + b_conv3)\n",
    "h_drop3 = tf.nn.dropout(h_conv3,keep_prob)\n",
    "\n",
    "# 上一层的输出是7*7*128，conv4\n",
    "W_conv4 = weight_variable([5,5,128,128])\n",
    "b_conv4 = bias_variable([128])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_drop3,W_conv4) + b_conv4)\n",
    "h_drop4 = tf.nn.dropout(h_conv4,keep_prob)\n",
    "\n",
    "# 上一层的输出是7*7*128，conv5\n",
    "W_conv5 = weight_variable([5,5,128,64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_drop4,W_conv5) + b_conv5)\n",
    "h_pool5 = tf.nn.max_pool(h_conv5,ksize=[1,1,1,1],strides=[1,1,1,1],padding='VALID')\n",
    "h_drop5 = tf.nn.dropout(h_pool5,keep_prob)\n",
    "\n",
    "# 上一层输出为7*7*64，矩阵变为一维\n",
    "h_drop5_flat = tf.reshape(h_drop5,[-1,7*7*64])\n",
    "\n",
    "# 上一层的输出是(7*7*64)，FC1\n",
    "W_fc1 = weight_variable([7*7*64,2048])\n",
    "b_fc1 = bias_variable([2048])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_drop5_flat,W_fc1) + b_fc1)\n",
    "h_drop6 = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 上一层的输出是（2048）,FC2\n",
    "W_fc2 = weight_variable([2048,1000])\n",
    "b_fc2 = bias_variable([1000])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_drop6,W_fc2) + b_fc2)\n",
    "h_drop7 = tf.nn.dropout(h_fc2,keep_prob)\n",
    "\n",
    "# 上一层的输出是1000，softmax\n",
    "W_fc3 = weight_variable([1000,10])\n",
    "b_fc3 = bias_variable([10])\n",
    "\n",
    "prediction = tf.nn.softmax(tf.matmul(h_drop7,W_fc3) + b_fc3)\n",
    "\n",
    "# 交叉熵\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = prediction,labels = y))\n",
    "# train_step\n",
    "train_step =tf.train.AdamOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "# 结果存放\n",
    "correct_pred = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "# 准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "# 初始化\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(11):\n",
    "        sess.run(tf.assign(lr,0.001 * (0.95 * epoch)))\n",
    "        for _ in range(n_batch):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch[0],y:batch[1],keep_prob:0.7})\n",
    "        \n",
    "        learning_rate = sess.run(lr)\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        print(\"Iter\" + str(epoch) + \"Test accuracy: \" + str(test_acc) + \"Train accuracy: \" + str(train_acc) + \"Learning_rate: \" + str(learning_rate))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
