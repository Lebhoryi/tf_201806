{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST READY\n",
      "CNN READY\n",
      "conv1_1/Relu  [None, 28, 28, 64]\n",
      "pool1_1/MaxPool  [None, 14, 14, 64]\n",
      "conv2_1/Relu  [None, 14, 14, 128]\n",
      "pool2_1/MaxPool  [None, 7, 7, 128]\n",
      "conv3_1/Relu  [None, 7, 7, 256]\n",
      "conv4_1/Relu  [None, 7, 7, 256]\n",
      "conv5_1/Relu  [None, 7, 7, 128]\n",
      "pool3_1/MaxPool  [None, 4, 4, 128]\n",
      "fc1_1/dropout/mul  [None, 1024]\n",
      "fc2_1/dropout/mul  [None, 1024]\n",
      "out_1/Add  [None, 10]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "softmax_cross_entropy_with_logits() got an unexpected keyword argument 'lables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b5b390298706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malex_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 前向传播的预测值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 交叉熵损失函数，参数分别为预测值_pred和实际label值y，reduce_mean为求平均loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0moptm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 梯度下降优化器\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tf.equal()对比预测值的索引和实际label的索引是否一样，一样返回True，不一样返回False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               instructions)\n\u001b[0;32m--> 250\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    252\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax_cross_entropy_with_logits() got an unexpected keyword argument 'lables'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('data/', one_hot=True)\n",
    "print(\"MNIST READY\")\n",
    "\n",
    "# 定义网络参数\n",
    "n_input = 784 # 输入的维度\n",
    "n_output = 10 # 标签的维度\n",
    "learning_rate = 0.001\n",
    "dropout = 0.75\n",
    "\n",
    "# 定义函数print_activations来显示网络每一层结构，展示每一个卷积层或池化层输出tensor的尺寸\n",
    "def print_activations(t):\n",
    "    print(t.op.name, '', t.get_shape().as_list())\n",
    "\n",
    "# 定义卷积操作\n",
    "def conv2d(input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding='SAME'), b)) # 参数分别指定了卷积核的尺寸、多少个channel、filter的个数即产生特征图的个数                                                                                       # 步长为1，即扫描全图像素,[1, 1, 1, 1]分别代表batch_size、h、w、c的stride\n",
    "# 定义池化操作\n",
    "def max_pool(input):\n",
    "    return tf.nn.max_pool(input, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # padding有两种选择：'SAME'（窗口滑动时，像素不够会自动补0）或'VALID'（不够就跳过）两种选择\n",
    "# 定义全连接操作\n",
    "def fc(input, w, b):\n",
    "    return tf.nn.relu(tf.add(tf.matmul(input, w), b)) # w*x+b，再通过非线性激活函数relu\n",
    "# 定义网络结构\n",
    "def alex_net(_input, _weights, _biases, _keep_prob):\n",
    "    _input_r = tf.reshape(_input, [-1, 28, 28, 1])  # 对图像做一个预处理，转换为tf支持的格式，即[n, h, w, c],-1是确定好其它3维后，让tf去推断剩下的1维\n",
    "\n",
    "    with tf.name_scope('conv1'):\n",
    "        _conv1 = conv2d(_input_r, _weights['wc1'], _biases['bc1'])\n",
    "        print_activations(_conv1) # 将这一层最后输出的tensor conv1的结构打印出来\n",
    "\n",
    "    # # 这里参数基本都是AlexNet论文中的推荐值，但目前其他经典卷积神经网络模型基本都放弃了LRN（主要是效果不明显），\n",
    "    # # 并且使用LRN也会让前馈、反馈的速度大大下降（整体速度降到1/3）\n",
    "    # with tf.name_scope('_lrn1'):\n",
    "    #     _lrn1 = tf.nn.lrn(_conv1, 4, bias=1.0, alpha=0.001/9, beta=0.75)\n",
    "\n",
    "    with tf.name_scope('pool1'):\n",
    "        _pool1 = max_pool(_conv1)\n",
    "        print_activations(_pool1)\n",
    "\n",
    "    with tf.name_scope('conv2'):\n",
    "        _conv2 = conv2d(_pool1, _weights['wc2'], _biases['bc2'])\n",
    "        print_activations(_conv2)\n",
    "\n",
    "    # with tf.name_scope('_lrn2'):\n",
    "    #     _lrn2 = tf.nn.lrn(_conv2, 4, bias=1.0, alpha=0.001/9, beta=0.75)\n",
    "\n",
    "    with tf.name_scope('pool2'):\n",
    "        _pool2 = max_pool(_conv2)\n",
    "        print_activations(_pool2)\n",
    "\n",
    "    with tf.name_scope('conv3'):\n",
    "        _conv3 = conv2d(_pool2, _weights['wc3'], _biases['bc3'])\n",
    "        print_activations(_conv3)\n",
    "\n",
    "    with tf.name_scope('conv4'):\n",
    "        _conv4 = conv2d(_conv3, _weights['wc4'], _biases['bc4'])\n",
    "        print_activations(_conv4)\n",
    "\n",
    "    with tf.name_scope('conv5'):\n",
    "        _conv5 = conv2d(_conv4, _weights['wc5'], _biases['bc5'])\n",
    "        print_activations(_conv5)\n",
    "\n",
    "    with tf.name_scope('pool3'):\n",
    "        _pool3 = max_pool(_conv5)\n",
    "        print_activations(_pool3)\n",
    "\n",
    "    _densel = tf.reshape(_pool3, [-1, _weights['wd1'].get_shape().as_list()[0]])  # 定义全连接层的输入，把pool2的输出做一个reshape，变为向量的形式\n",
    "\n",
    "    # pool_shape = _pool3.get_shape().as_list()\n",
    "    # nodes = pool_shape[1] * pool_shape[2] * pool_shape[3]\n",
    "\n",
    "    with tf.name_scope('fc1'):\n",
    "        _fc1 = fc(_densel, _weights['wd1'], _biases['bd1'])\n",
    "        _fc1_drop = tf.nn.dropout(_fc1, _keep_prob) # 为了减轻过拟合，使用Dropout层\n",
    "        print_activations(_fc1_drop)\n",
    "\n",
    "    with tf.name_scope('fc2'):\n",
    "        _fc2 = fc(_fc1_drop, _weights['wd2'], _biases['bd2'])\n",
    "        _fc2_drop = tf.nn.dropout(_fc2, _keep_prob)\n",
    "        print_activations(_fc2_drop)\n",
    "\n",
    "    with tf.name_scope('out'):\n",
    "        _out = tf.add(tf.matmul(_fc2_drop, _weights['wd3']), _biases['bd3'])\n",
    "        print_activations(_out)\n",
    "\n",
    "    return _out\n",
    "print(\"CNN READY\")\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input]) # 用placeholder先占地方，样本个数不确定为None\n",
    "y = tf.placeholder(tf.float32, [None, n_output]) # 用placeholder先占地方，样本个数不确定为None\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 存储所有的网络参数\n",
    "weights = {\n",
    "    # 使用截断的正态分布（标准差0.1）初始化卷积核的参数kernel，卷积核大小为3*3，channel为1，个数64\n",
    "    'wc1': tf.Variable(tf.truncated_normal([3, 3, 1, 64], dtype=tf.float32, stddev=0.1), name='weights1'),\n",
    "    'wc2': tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=0.1), name='weights2'),\n",
    "    'wc3': tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32, stddev=0.1), name='weights3'),\n",
    "    'wc4': tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32, stddev=0.1), name='weights4'),\n",
    "    'wc5': tf.Variable(tf.truncated_normal([3, 3, 256, 128], dtype=tf.float32, stddev=0.1), name='weights5'),\n",
    "    'wd1': tf.Variable(tf.truncated_normal([4*4*128, 1024], dtype=tf.float32, stddev=0.1), name='weights_fc1'),\n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024], dtype=tf.float32, stddev=0.1), name='weights_fc2'),\n",
    "    'wd3': tf.Variable(tf.random_normal([1024, n_output], dtype=tf.float32, stddev=0.1), name='weights_output')\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32), trainable=True, name='biases1'),\n",
    "    'bc2': tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases2'),\n",
    "    'bc3': tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases3'),\n",
    "    'bc4': tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32), trainable=True, name='biases4'),\n",
    "    'bc5': tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases5'),\n",
    "    'bd1': tf.Variable(tf.constant(0.0, shape=[1024], dtype=tf.float32), trainable=True, name='biases_fc1'),\n",
    "    'bd2': tf.Variable(tf.constant(0.0, shape=[1024], dtype=tf.float32), trainable=True, name='biases_fc2'),\n",
    "    'bd3': tf.Variable(tf.constant(0.0, shape=[n_output], dtype=tf.float32), trainable=True, name='biases_output')\n",
    "}\n",
    "\n",
    "pred = alex_net(x, weights, biases, keep_prob) # 前向传播的预测值\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y)) # 交叉熵损失函数，参数分别为预测值_pred和实际label值y，reduce_mean为求平均loss\n",
    "optm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # 梯度下降优化器\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # tf.equal()对比预测值的索引和实际label的索引是否一样，一样返回True，不一样返回False\n",
    "accuracy = tf.reduce_mean(tf.cast(corr, tf.float32)) # 将pred即True或False转换为1或0,并对所有的判断结果求均值\n",
    "# 初始化所有参数\n",
    "init = tf.global_variables_initializer()\n",
    "print(\"FUNCTIONS READY\")\n",
    "\n",
    "# 上面神经网络结构定义好之后，下面定义一些超参数\n",
    "training_epochs = 1000 # 所有样本迭代1000次\n",
    "batch_size = 1 # 每进行一次迭代选择50个样本\n",
    "display_step = 10\n",
    "\n",
    "sess = tf.Session() # 定义一个Session\n",
    "sess.run(init) # 在sess里run一下初始化操作\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    start_time = time.time()\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size) # 逐个batch的去取数据\n",
    "        # 获取批数据\n",
    "        sess.run(optm, feed_dict={x: batch_xs, y: batch_ys, keep_prob:dropout})\n",
    "        avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob:1.0})/total_batch\n",
    "    if epoch % display_step == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})\n",
    "        test_accuracy = sess.run(accuracy, feed_dict={x: mnist.test.images, y: mnist.test.labels, keep_prob:1.0})\n",
    "        print(\"Epoch: %03d/%03d cost: %.9f TRAIN ACCURACY: %.3f TEST ACCURACY: %.3f\" % (epoch, training_epochs, avg_cost, train_accuracy, test_accuracy))\n",
    "    # 计算每轮迭代的平均耗时mn和标准差sd，并显示\n",
    "    duration = time.time() - start_time\n",
    "    print('%s: step %d, duration = %.3f' % (datetime.now(), epoch, duration))\n",
    "\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
